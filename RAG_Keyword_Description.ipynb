{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Document ID', 'Topic', 'Sub-topic', 'Text Content', 'Source'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file and inspect its columns\n",
    "qa_df = pd.read_csv('C:/Users/dinon/Desktop/Summary/biology_information_retrieval_sample.csv', encoding='ISO-8859-1')\n",
    "print(qa_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the notes dataset\n",
    "notes_df = pd.read_csv('C:/Users/dinon/Desktop/Summary/biology_information_retrieval_sample.csv', encoding='ISO-8859-1')  # Update with the correct file path\n",
    "notes_content = notes_df['Text Content'].tolist()\n",
    "notes_topics = notes_df['Topic'].tolist()\n",
    "notes_subtopics = notes_df['Sub-topic'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['longtext', 'summary', 'keywords'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file and inspect its columns\n",
    "qa_df = pd.read_csv('C:/Users/dinon/Desktop/Summary/bio_summary_key.csv', encoding='ISO-8859-1')\n",
    "print(qa_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the summarization dataset\n",
    "summary_df = pd.read_csv('C:/Users/dinon/Desktop/Summary/bio_summary_key.csv', encoding='ISO-8859-1')\n",
    "long_texts = summary_df['longtext'].tolist()\n",
    "summaries = summary_df['summary'].tolist()\n",
    "keywords = summary_df['keywords'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\python312\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\python312\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\python312\\lib\\site-packages (from sentence-transformers) (4.45.2)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\python312\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\python312\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\python312\\lib\\site-packages (from sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\python312\\lib\\site-packages (from sentence-transformers) (0.25.2)\n",
      "Requirement already satisfied: Pillow in c:\\python312\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\python312\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\dinon\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.0)\n",
      "Requirement already satisfied: filelock in c:\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dinon\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load embedding model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for summarization dataset (long texts)\n",
    "summary_embeddings = embedder.encode(long_texts)\n",
    "\n",
    "# Generate embeddings for notes dataset\n",
    "notes_embeddings = embedder.encode(notes_content)\n",
    "\n",
    "# Combine all content and embeddings for FAISS indexing\n",
    "all_content = long_texts + notes_content\n",
    "all_embeddings = np.concatenate([summary_embeddings, notes_embeddings], axis=0)\n",
    "\n",
    "# Convert embeddings to a float32 NumPy array\n",
    "all_embeddings_array = np.array(all_embeddings).astype(\"float32\")\n",
    "\n",
    "# Create and populate the FAISS index\n",
    "common_index = faiss.IndexFlatL2(all_embeddings_array.shape[1])\n",
    "common_index.add(all_embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_content(query, k=5):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedder.encode([query]).astype(\"float32\")\n",
    "\n",
    "    # Search the FAISS index for similar content\n",
    "    distances, indices = common_index.search(query_embedding, k)\n",
    "    \n",
    "    # Retrieve and categorize results\n",
    "    results = []\n",
    "    for i in indices[0]:\n",
    "        if i < len(long_texts):  # Entry from summarization dataset\n",
    "            results.append({\n",
    "                'type': 'summary',\n",
    "                'long_text': long_texts[i],\n",
    "                'summary': summaries[i],\n",
    "                'keyword': keywords[i]\n",
    "            })\n",
    "        else:  # Entry from notes dataset\n",
    "            idx = i - len(long_texts)\n",
    "            results.append({\n",
    "                'type': 'note',\n",
    "                'topic': notes_topics[idx],\n",
    "                'sub_topic': notes_subtopics[idx],\n",
    "                'content': notes_content[idx]\n",
    "            })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Path to your fine-tuned model\n",
    "model_path = 'C:/Users/dinon/Desktop/Summary/flan_t5_finetuned_model-20241114T172316Z-001/flan_t5_finetuned_model'  # Update with the correct path\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(query, max_words=100):\n",
    "    # Retrieve similar content from the common index\n",
    "    similar_content = retrieve_similar_content(query, k=5)\n",
    "    \n",
    "    # Construct context from retrieved content\n",
    "    context = \"\"\n",
    "    for item in similar_content:\n",
    "        if item['type'] == 'summary':\n",
    "            context += f\"Keyword: {item['keyword']}\\nLong Text: {item['long_text']}\\n\\n\"\n",
    "        elif item['type'] == 'note':\n",
    "            context += f\"Topic: {item['topic']} - {item['sub_topic']}\\nContent: {item['content']}\\n\\n\"\n",
    "    \n",
    "    # Generate the prompt with query and context for summarization\n",
    "    prompt = f\"Summarize the following content related to '{query}':\\n{context}\\nSummary:\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=max_words, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample query for summarization\n",
    "sample_query = \"Importance of biodiversity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "Biodiversity is the diversity of living organisms from all sources including terrestrial, marine, and other aquatic ecosystems and their ecological interactions with the environment. Genetic diversity is the genetic variation that exists both within and among species. b. Species diversity is the variation that can be recognized among different species. c. Ecosystem diversity is the variety of habitats, living communities and ecological processes in the living world. The importance and values of biodiversity The individual components of biodiversitygenes, species, and ecosystems provide the human society with a wide array of goods and services.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Summary:\")\n",
    "print(generate_summary(sample_query, max_words=150))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
